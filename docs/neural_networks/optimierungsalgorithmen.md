# Optimierungsalgorithmen
## Gradient Descent, das Gradientenverfahren
Bei diesem Verfahren geht man, von einem Startpunkt anfangend, in Richtung des negativen Gradienten
bis man an einem Punkt angekommen ist, an dem es nicht mehr tiefer geht.
Dazu werden kleine Schritte in die Richtung des Minimums gemacht, die bei uns als Lernrate beizeichnet wird. 
Diese Lernrate darf nicht zu groß gewählt werden, damit man an dem tiefsten Punkt überhaupt ankommen kann.
Bei einer zu großen Lernrate kann man sich das so vorstellen wie bei einem Tal. Man steht auf einem Berg neben
dem Tal, möchte aber in das Tal. Wenn man sich jetzt aber nur in zu großen Schritten bewegen kann, springt man immer 
von einer Seite des Tals auf die andere, erreicht aber nie wirklich sein Ziel.

